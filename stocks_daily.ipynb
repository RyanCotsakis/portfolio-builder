{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3146c9fd-179b-4911-889f-5e11da8dc073",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-fbc5e49cd709>:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b6f1790-0eda-4f61-b93e-1782306194a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_ticker(ticker):\n",
    "    url = f\"https://query1.finance.yahoo.com/v7/finance/download/{ticker}?period1=1483228800&period2=1767225600&interval=1d&events=history&includeAdjustedClose=true\"\n",
    "    print(\"Fetching data for \" + ticker)\n",
    "    headers = {\"User-Agent\" : \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"}\n",
    "    response = requests.get(url, headers=headers)\n",
    "    with open(\"temp.csv\", \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "    df = pd.read_csv(\"temp.csv\")\n",
    "    os.remove(\"temp.csv\")  # Remove the temporary CSV file\n",
    "    return df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "970ac1b4-ff02-406e-95c4-f17f5391d330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find common dates across all DataFrames\n",
    "def preprocess_dfs(tickers):\n",
    "    raw_dfs = [df_from_ticker(ticker) for ticker in tickers]\n",
    "    print(\"Done.\")\n",
    "    common_dates = set(raw_dfs[0][\"Date\"])\n",
    "    for df in raw_dfs[1:]:\n",
    "        common_dates.intersection_update(df[\"Date\"])\n",
    "\n",
    "    # Sort the common dates in descending order\n",
    "    common_dates = sorted(common_dates)\n",
    "    print(\"Using \" + str(len(common_dates)) + \" days.\")\n",
    "    print(\"Until \" + common_dates[-1] + \".\")\n",
    "\n",
    "    # Filter DataFrames to keep only common dates\n",
    "    filtered_dfs = []\n",
    "    for j, df in enumerate(raw_dfs):\n",
    "        i = 0\n",
    "        while (i < df.shape[0]):\n",
    "            if df[\"Date\"].iloc[i] not in common_dates:\n",
    "                df = df.iloc[list(set(range(df.shape[0])) - set([i]))]\n",
    "            else:\n",
    "                i += 1\n",
    "        df = df.reset_index(drop=True)\n",
    "        filtered_dfs.append(df[[\"High\", \"Low\", \"Close\", \"Volume\"]])\n",
    "    for i, df in enumerate(filtered_dfs):\n",
    "        filtered_dfs[i] = np.log(df)\n",
    "\n",
    "    for i, df in enumerate(filtered_dfs):\n",
    "        df.columns = [tickers[i] + '_' + col if col != 'Date' else col for col in df.columns]\n",
    "\n",
    "    # Concatenate all dfs\n",
    "    concatenated_df = pd.concat(filtered_dfs, axis = 1)\n",
    "    print(\"Using {} features.\".format(concatenated_df.shape[1]))\n",
    "    return(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b28120fc-9df4-4828-b8ab-566e78a2f73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data(preprocessed, filt, decay_constant=0.33):\n",
    "    n = preprocessed.shape[0]\n",
    "    Y = np.zeros((n, len(filt)))\n",
    "    Y_vectors = preprocessed[[ticker + \"_Close\" for ticker in filt]]\n",
    "    Y[n-1,] = Y_vectors.iloc[n-1]\n",
    "    for i in range(n - 2, -1, -1):\n",
    "        Y[i,] = (1-decay_constant)*Y_vectors.iloc[i+1] + decay_constant*Y[i+1,]\n",
    "    # drop the last observation\n",
    "    Y = Y[:-1,] - Y_vectors.drop(index=n-1)\n",
    "    X = preprocessed.drop(index=n-1)\n",
    "    n -= 1\n",
    "    \n",
    "    print(\"Training on X: \" + str(X.shape))\n",
    "    print(\"Training on Y: \" + str(Y.shape))\n",
    "    return (X, Y)\n",
    "\n",
    "def normalize(X, Y=None):\n",
    "    X_means = np.mean(X, axis=0)\n",
    "    X_stds = np.std(X, axis=0)\n",
    "    X_normalized = (X - X_means) / X_stds\n",
    "    if Y is None:\n",
    "        return X_normalized\n",
    "    Y_means = np.mean(Y, axis=0)\n",
    "    Y_normalized = Y - Y_means\n",
    "    return (X_normalized, Y_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4098b2f-d482-4a92-86b0-14503d8cd413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No GPU available, training on CPU; consider making n_epochs very small.\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08a2e105-22f1-4ebd-b379-067859e3c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PortfolioBuilder(nn.Module):\n",
    "    def __init__(self, n_input, n_output, n_hidden, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.n_hidden = n_hidden\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.h1 = nn.Linear(n_input, n_input)\n",
    "        self.h2 = nn.Linear(n_input, n_input)\n",
    "        self.lstm = nn.LSTM(n_input, n_hidden, n_layers)\n",
    "        self.out = nn.Linear(n_hidden, n_output)\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.3)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        x = self.dropout(self.h1(x))\n",
    "        x = self.dropout(torch.tanh(self.h2(x)))\n",
    "        x, hidden = self.lstm(x.unsqueeze(0), hidden)\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.squeeze(0)\n",
    "        x = F.softmax(self.out(x), dim=1)\n",
    "        return(x, hidden)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data # match the datatype\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        return hidden if batch_size > 1 else tuple([h.view(self.n_layers, self.n_hidden) for h in hidden])\n",
    "\n",
    "def returns(portfolio, log_changes, sharpe=False):\n",
    "    if len(portfolio.shape) == 1:\n",
    "        portfolio = portfolio.unsqueeze(0)\n",
    "        log_changes = log_changes.unsqueeze(0)\n",
    "    if sharpe:\n",
    "        Sigma = torch.Tensor(np.array(Y.cov()))\n",
    "        aSigmaa = (torch.matmul(portfolio[:,:-1], Sigma) * portfolio[:,:-1]).sum(axis=1)\n",
    "        neg_loss_sharpe = -((portfolio[:,:-1] * log_changes).sum(axis=1)/torch.sqrt(aSigmaa)).mean()\n",
    "        return(neg_loss_sharpe)\n",
    "    else:\n",
    "        neg_loss = -(portfolio[:,:-1] * log_changes).sum(axis=1).mean()\n",
    "        return(neg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22fed406-bdf5-4c6f-9959-a74c440f0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, Y, n_epochs, batch_size=10, lr=0.001, clip=5, test_size=0.2, file=None):\n",
    "    start_at = X.shape[0] - batch_size * (X.shape[0]//batch_size)\n",
    "    all_indices = np.array(range(start_at, X.shape[0]))\n",
    "    all_indices = all_indices.reshape(batch_size,-1)\n",
    "    n = all_indices.shape[1]\n",
    "    use_for_test = [False if i/n < 1- test_size else True for i in range(n)]\n",
    "    \n",
    "    optimizer = torch.optim.Adam(rnn.parameters(), lr = lr)\n",
    "    \n",
    "    min_test_loss = float(\"inf\")\n",
    "    all_losses = []\n",
    "    for epoch in range(n_epochs):\n",
    "#         np.random.shuffle(train_indices)\n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        train_loss = 0\n",
    "        test_loss = 0\n",
    "        for i in range(n):\n",
    "            inds = all_indices[:,i]\n",
    "            if use_for_test[i]:\n",
    "                rnn.eval()\n",
    "            else:\n",
    "                rnn.train()\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "            x = torch.Tensor(np.array(X.iloc[inds])).to(torch.float32)\n",
    "            y = torch.Tensor(np.array(Y.iloc[inds])).to(torch.float32)\n",
    "                \n",
    "\n",
    "            prediction, hidden = rnn(x, hidden)\n",
    "            hidden = tuple([h.data for h in hidden])\n",
    "#             print(hidden)\n",
    "            loss = criterion(prediction, y)\n",
    "    \n",
    "            if use_for_test[i]:\n",
    "                test_loss += loss\n",
    "            else:\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(rnn.parameters(), clip)\n",
    "                optimizer.step()\n",
    "                train_loss += loss\n",
    "        train_loss = train_loss.item() / (n - np.sum(use_for_test))\n",
    "        test_loss = test_loss.item() / np.sum(use_for_test)\n",
    "        all_losses.append((train_loss, test_loss))\n",
    "        if epoch % 10 == 9:\n",
    "            print(\"Epochs: \" + str(epoch+1))\n",
    "            print(\"Train / Test loss: \" + \"{:.6f}\".format(train_loss) + \" / {:.6f}\".format(test_loss))\n",
    "        if test_loss < min_test_loss and file is not None:\n",
    "            print(\"Saving at epoch {}. Test loss of {}.\".format(epoch+1, test_loss))\n",
    "            min_test_loss = test_loss\n",
    "            torch.save({'model_state_dict': rnn.state_dict(),\n",
    "                        'loss': test_loss}, file)\n",
    "\n",
    "    plt.plot(list(range(n_epochs)), [loss[0] for loss in all_losses])\n",
    "    plt.plot(list(range(n_epochs)), [loss[1] for loss in all_losses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35f52b4b-58de-48d5-8960-f5afa5f15d29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching data for META\n",
      "Fetching data for IBM\n",
      "Fetching data for GOOG\n",
      "Fetching data for MSFT\n",
      "Fetching data for NVDA\n",
      "Fetching data for SPY\n",
      "Fetching data for INTC\n",
      "Fetching data for TSLA\n",
      "Fetching data for BTC-USD\n",
      "Done.\n",
      "Using 1794 days.\n",
      "Until 2024-02-20.\n",
      "Using 36 features.\n"
     ]
    }
   ],
   "source": [
    "tickers = [\"META\", \"IBM\", \"GOOG\", \"MSFT\", \"NVDA\", \"SPY\", \"INTC\", \"TSLA\", \"BTC-USD\"]  # Example list of stock codes\n",
    "filt = [\"IBM\", \"NVDA\", \"GOOG\"] # the stocks to include in the portfolio\n",
    "\n",
    "preprocessed = preprocess_dfs(tickers)\n",
    "model_file = 'model.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1d6dd3-999b-4089-a946-dbf7264e2223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PortfolioBuilder(\n",
      "  (h1): Linear(in_features=36, out_features=36, bias=True)\n",
      "  (h2): Linear(in_features=36, out_features=36, bias=True)\n",
      "  (lstm): LSTM(36, 3)\n",
      "  (out): Linear(in_features=3, out_features=4, bias=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "rnn = PortfolioBuilder(preprocessed.shape[1], len(filt) + 1, len(filt))\n",
    "print(rnn)\n",
    "\n",
    "new_model = False # train a new model?\n",
    "if new_model:\n",
    "    X, Y = get_training_data(preprocessed, filt, decay_constant=0.33)\n",
    "    X, Y = normalize(X, Y)\n",
    "\n",
    "    sharpe = False # use the sharpe ratio or maximize profits?\n",
    "    criterion = lambda predicted, target: returns(predicted, target, sharpe)\n",
    "\n",
    "    train(X, Y, n_epochs=120, batch_size=10, test_size=0.2, file=model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc696e56-4cbc-44c9-8c3e-a5b940ebe458",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using all 1794 days.\n",
      "Total assets: 3700 EUR\n",
      "Averages: {'IBM': 731.9387734858191, 'NVDA': 970.2797648646266, 'GOOG': 959.1906892681865, 'CASH': 1038.590770261268}\n",
      "Portfolio: {'IBM': 6.907069, 'NVDA': 15.363761, 'GOOG': 32.618393, 'CASH': 3645.1108}\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if model_file is not None:\n",
    "    checkpoint = torch.load(model_file)\n",
    "    rnn.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "hidden = rnn.init_hidden()\n",
    "\n",
    "X = normalize(preprocessed)\n",
    "n = X.shape[0]\n",
    "print(\"Using all {} days.\".format(n))\n",
    "\n",
    "all_predictions = np.zeros(len(filt)+1)\n",
    "for i in range(n):\n",
    "    x = torch.Tensor(np.array(X.iloc[i])).to(torch.float32)\n",
    "    prediction, hidden = rnn(x, hidden)\n",
    "    all_predictions += np.array(prediction.squeeze().detach())\n",
    "\n",
    "total_assets = 3700\n",
    "print(\"Total assets: {} EUR\".format(total_assets))\n",
    "prediction = np.array(prediction.detach()).flatten() * total_assets\n",
    "all_predictions = all_predictions / n * total_assets\n",
    "\n",
    "portfolio = {}\n",
    "averages = {}\n",
    "for i in range(len(filt)):\n",
    "    portfolio[filt[i]] = prediction[i]\n",
    "    averages[filt[i]] = all_predictions[i]\n",
    "portfolio[\"CASH\"] = prediction[len(filt)]\n",
    "averages[\"CASH\"] = all_predictions[len(filt)]\n",
    "print(\"Averages: \" + str(averages))\n",
    "print(\"Portfolio: \" + str(portfolio))\n",
    "\n",
    "plt.bar(portfolio.keys(), portfolio.values())\n",
    "plt.title(\"Today's portfolio\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
